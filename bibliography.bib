@article{Imbens1994,
	ISSN = {00129682, 14680262},
	URL = {http://www.jstor.org/stable/2951620},
	author = {Guido W. Imbens and Joshua D. Angrist},
	journal = {Econometrica},
	number = {2},
	pages = {467--475},
	publisher = {[Wiley, Econometric Society]},
	title = {Identification and Estimation of Local Average Treatment Effects},
	urldate = {2023-10-13},
	volume = {62},
	year = {1994}
}

@article{Angrist1996,
	ISSN = {01621459},
	URL = {http://www.jstor.org/stable/2291629},
	abstract = {We outline a framework for causal inference in settings where assignment to a binary treatment is ignorable, but compliance with the assignment is not perfect so that the receipt of treatment is nonignorable. To address the problems associated with comparing subjects by the ignorable assignment-an "intention-to-treat analysis"-we make use of instrumental variables, which have long been used by economists in the context of regression models with constant treatment effects. We show that the instrumental variables (IV) estimand can be embedded within the Rubin Causal Model (RCM) and that under some simple and easily interpretable assumptions, the IV estimand is the average causal effect for a subgroup of units, the compliers. Without these assumptions, the IV estimand is simply the ratio of intention-to-treat causal estimands with no interpretation as an average causal effect. The advantages of embedding the IV approach in the RCM are that it clarifies the nature of critical assumptions needed for a causal interpretation, and moreover allows us to consider sensitivity of the results to deviations from key assumptions in a straightforward manner. We apply our analysis to estimate the effect of veteran status in the Vietnam era on mortality, using the lottery number that assigned priority for the draft as an instrument, and we use our results to investigate the sensitivity of the conclusions to critical assumptions.},
	author = {Joshua D. Angrist and Guido W. Imbens and Donald B. Rubin},
	journal = {Journal of the American Statistical Association},
	number = {434},
	pages = {444--455},
	publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
	title = {Identification of Causal Effects Using Instrumental Variables},
	urldate = {2023-10-16},
	volume = {91},
	year = {1996}
}



@article{Angrist1995,
	author = {Joshua D. Angrist and Guido W. Imbens},
	title = {Two-Stage Least Squares Estimation of Average Causal Effects in Models with Variable Treatment Intensity},
	journal = {Journal of the American Statistical Association},
	volume = {90},
	number = {430},
	pages = {431-442},
	year = {1995},
	publisher = {Taylor & Francis},
	doi = {10.1080/01621459.1995.10476535}, }

@article{Angrist2001,
	Author = {Angrist, Joshua D. and Krueger, Alan B.},
	Title = {Instrumental Variables and the Search for Identification: From Supply and Demand to Natural Experiments},
	Journal = {Journal of Economic Perspectives},
	Volume = {15},
	Number = {4},
	Year = {2001},
	Month = {December},
	Pages = {69-85},
	DOI = {10.1257/jep.15.4.69},
	URL = {https://www.aeaweb.org/articles?id=10.1257/jep.15.4.69}}


@article{Abadie2018,
	author = {Abadie, Alberto and Cattaneo, Matias D.},
	title = {Econometric Methods for Program Evaluation},
	journal = {Annual Review of Economics},
	volume = {10},
	number = {1},
	pages = {465-503},
	year = {2018},
	doi = {10.1146/annurev-economics-080217-053402},
	
	URL = { 
	
	https://doi.org/10.1146/annurev-economics-080217-053402
	
	
	
	},
	eprint = { 
	
	https://doi.org/10.1146/annurev-economics-080217-053402
	
	
	
	}
	,
	abstract = { Program evaluation methods are widely applied in economics to assess the effects of policy interventions and other treatments of interest. In this article, we describe the main methodological frameworks of the econometrics of program evaluation. In the process, we delineate some of the directions along which this literature is expanding, discuss recent developments, and highlight specific areas where new research may be particularly fruitful. }
}


@article{Imbens2014,
	author = {Guido W. Imbens},
	title = {{Instrumental Variables: An Econometrician’s Perspective}},
	volume = {29},
	journal = {Statistical Science},
	number = {3},
	publisher = {Institute of Mathematical Statistics},
	pages = {323 -- 358},
	keywords = {noncompliance, potential outcomes, randomized experiments, selection models, Simultaneous equations models},
	year = {2014},
	doi = {10.1214/14-STS480},
	URL = {https://doi.org/10.1214/14-STS480}
}

@article{Angrist2000,
	ISSN = {00346527, 1467937X},
	URL = {http://www.jstor.org/stable/2566964},
	abstract = {In markets where prices are determined by the intersection of supply and demand curves, standard identification results require the presence of instruments that shift one curve but not the other. These results are typically presented in the context of linear models with fixed coefficients and additive residuals. The first contribution of this paper is an investigation of the consequences of relaxing both the linearity and the additivity assumption for the interpretation of linear instrumental variables estimators. Without these assumptions, the standard linear instrumental variables estimator identifies a weighted average of the derivative of the behavioural relationship of interest. A second contribution is the formulation of critical identifying assumptions in terms of demand and supply at different prices and instruments, rather than in terms of functional-form specific residuals. Our approach to the simultaneous equations problem and the average-derivative interpretation of instrumental variables estimates is illustrated by estimating the demand for fresh whiting at the Fulton fish market. Strong and credible instruments for identification of this demand function are available in the form of weather conditions at sea.},
	author = {Joshua D. Angrist and Kathryn Graddy and Guido W. Imbens},
	journal = {The Review of Economic Studies},
	number = {3},
	pages = {499--527},
	publisher = {[Oxford University Press, Review of Economic Studies, Ltd.]},
	title = {The Interpretation of Instrumental Variables Estimators in Simultaneous Equations Models with an Application to the Demand for Fish},
	urldate = {2023-10-16},
	volume = {67},
	year = {2000}
}

@article{Mogstad2021,
	Author = {Mogstad, Magne and Torgovitsky, Alexander and Walters, Christopher R.},
	Title = {The Causal Interpretation of Two-Stage Least Squares with Multiple Instrumental Variables},
	Journal = {American Economic Review},
	Volume = {111},
	Number = {11},
	Year = {2021},
	Month = {November},
	Pages = {3663-98},
	DOI = {10.1257/aer.20190221},
	URL = {https://www.aeaweb.org/articles?id=10.1257/aer.20190221}}

@article{Heckman2018,
	author = {Heckman, James J. and Pinto, Rodrigo},
	title = {Unordered Monotonicity},
	journal = {Econometrica},
	volume = {86},
	number = {1},
	pages = {1-35},
	keywords = {Instrumental variables, monotonicity, revealed preference, Generalized Roy model, binary matrices, discrete choice, selection bias, identification, discrete mixtures},
	doi = {https://doi.org/10.3982/ECTA13777},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA13777},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.3982/ECTA13777},
	abstract = {This paper defines and analyzes a new monotonicity condition for the identification of counterfactuals and treatment effects in unordered discrete choice models with multiple treatments, heterogeneous agents, and discrete-valued instruments. Unordered monotonicity implies and is implied by additive separability of choice of treatment equations in terms of observed and unobserved variables. These results follow from properties of binary matrices developed in this paper. We investigate conditions under which unordered monotonicity arises as a consequence of choice behavior. We characterize IV estimators of counterfactuals as solutions to discrete mixture problems.},
	year = {2018}
}

@article{Torgovitsky2021,
	Author = {Mogstad, Magne and Torgovitsky, Alexander and Walters, Christopher R.},
	Title = {The Causal Interpretation of Two-Stage Least Squares with Multiple Instrumental Variables},
	Journal = {American Economic Review},
	Volume = {111},
	Number = {11},
	Year = {2021},
	Month = {November},
	Pages = {3663-98},
	DOI = {10.1257/aer.20190221},
	URL = {https://www.aeaweb.org/articles?id=10.1257/aer.20190221}}

@techreport{blandhol2022tsls,
	title={When is TSLS actually late?},
	author={Blandhol, Christine and Bonney, John and Mogstad, Magne and Torgovitsky, Alexander},
	year={2022},
	institution={National Bureau of Economic Research}
}

@misc{Stoczynski2022,
	title={When Should We (Not) Interpret Linear IV Estimands as LATE?}, 
	author={Tymon Słoczyński},
	year={2022},
	eprint={2011.06695},
	archivePrefix={arXiv},
	primaryClass={econ.EM}
}

@techreport{kolesa2013estimation,
	title={Estimation in an instrumental variables model with treatment effect heterogeneity},
	author={Kolesár, Michal and others},
	year={2013}
}

@book{angrist2009mostly,
	title={Mostly harmless econometrics: An empiricist's companion},
	author={Angrist, Joshua D and Pischke, J{\"o}rn-Steffen},
	year={2009},
	publisher={Princeton university press}
}


@article{heckman2006understanding,
	title={Understanding instrumental variables in models with essential heterogeneity},
	author={Heckman, James J and Urzua, Sergio and Vytlacil, Edward},
	journal={The review of economics and statistics},
	volume={88},
	number={3},
	pages={389--432},
	year={2006},
	publisher={The MIT Press}
}


@article{coussens2021improving,
	title={Improving inference from simple instruments through compliance estimation},
	author={Coussens, Stephen and Spiess, Jann},
	journal={arXiv preprint arXiv:2108.03726},
	year={2021}
}

@article{mogstad2018using,
	title={Using instrumental variables for inference about policy relevant treatment parameters},
	author={Mogstad, Magne and Santos, Andres and Torgovitsky, Alexander},
	journal={Econometrica},
	volume={86},
	number={5},
	pages={1589--1619},
	year={2018},
	publisher={Wiley Online Library}
}


@article{Escanciano2023,
	title = {Robust Identification in Regression Discontinuity Designs with Covariates},
	author = {Escanciano, Juan Carlos and Caetano, Carolina and Caetano, Gregorio},
	year = {2023}
}

@article{heckman2005structural,
	title={Structural equations, treatment effects, and econometric policy evaluation 1},
	author={Heckman, James J and Vytlacil, Edward},
	journal={Econometrica},
	volume={73},
	number={3},
	pages={669--738},
	year={2005},
	publisher={Wiley Online Library}
}

@article{Roth2023,
	title = {What’s trending in difference-in-differences? A synthesis of the recent econometrics literature},
	journal = {Journal of Econometrics},
	volume = {235},
	number = {2},
	pages = {2218-2244},
	year = {2023},
	issn = {0304-4076},
	doi = {https://doi.org/10.1016/j.jeconom.2023.03.008},
	url = {https://www.sciencedirect.com/science/article/pii/S0304407623001318},
	author = {Jonathan Roth and Pedro H.C. Sant’Anna and Alyssa Bilinski and John Poe},
	keywords = {Difference-in-differences, Causal Inference, Staggered Treatment timing, Sensitivity Analysis, Clustering, Parallel trends, Treatment Effect Heterogeneity},
	abstract = {This paper synthesizes recent advances in the econometrics of difference-in-differences (DiD) and provides concrete recommendations for practitioners. We begin by articulating a simple set of “canonical” assumptions under which the econometrics of DiD are well-understood. We then argue that recent advances in DiD methods can be broadly classified as relaxing some components of the canonical DiD setup, with a focus on (i) multiple periods and variation in treatment timing, (ii) potential violations of parallel trends, or (iii) alternative frameworks for inference. Our discussion highlights the different ways that the DiD literature has advanced beyond the canonical model, and helps to clarify when each of the papers will be relevant for empirical work. We conclude by discussing some promising areas for future research.}
}

@article{duflo2001schooling,
	title={Schooling and labor market consequences of school construction in Indonesia: Evidence from an unusual policy experiment},
	author={Duflo, Esther},
	journal={American economic review},
	volume={91},
	number={4},
	pages={795--813},
	year={2001},
	publisher={American Economic Association}
}

@article{Chaisemartin2017,
	author = {de Chaisemartin, C and D’HaultfŒuille, X},
	title = "{Fuzzy Differences-in-Differences}",
	journal = {The Review of Economic Studies},
	volume = {85},
	number = {2},
	pages = {999-1028},
	year = {2017},
	month = {08},
	abstract = "{Difference-in-differences (DID) is a method to evaluate the effect of a treatment. In its basic version, a “control group” is untreated at two dates, whereas a “treatment group” becomes fully treated at the second date. However, in many applications of the DID method, the treatment rate only increases more in the treatment group. In such fuzzy designs, a popular estimator of the treatment effect is the DID of the outcome divided by the DID of the treatment. We show that this ratio identifies a local average treatment effect only if the effect of the treatment is stable over time, and if the effect of the treatment is the same in the treatment and in the control group. We then propose two alternative estimands that do not rely on any assumption on treatment effects, and that can be used when the treatment rate does not change over time in the control group. We prove that the corresponding estimators are asymptotically normal. Finally, we use our results to reassess the returns to schooling in Indonesia.}",
	issn = {0034-6527},
	doi = {10.1093/restud/rdx049},
	url = {https://doi.org/10.1093/restud/rdx049},
	eprint = {https://academic.oup.com/restud/article-pdf/85/2/999/24473453/rdx049.pdf},
}

@article{callaway2021difference,
	title={Difference-in-differences with multiple time periods},
	author={Callaway, Brantly and Sant’Anna, Pedro HC},
	journal={Journal of econometrics},
	volume={225},
	number={2},
	pages={200--230},
	year={2021},
	publisher={Elsevier}
}

@article{Vytlacil2002,
	ISSN = {00129682, 14680262},
	URL = {http://www.jstor.org/stable/2692171},
	author = {Edward Vytlacil},
	journal = {Econometrica},
	number = {1},
	pages = {331--341},
	publisher = {[Wiley, Econometric Society]},
	title = {Independence, Monotonicity, and Latent Index Models: An Equivalence Result},
	urldate = {2023-10-16},
	volume = {70},
	year = {2002}
}

@article{Hudson2017,
	title={Interpreting instrumented difference-in-differences},
	author={Hudson, Sally and Hull, Peter and Liebersohn, Jack},
	year={2017}
}

@article{Cornelisesen2016,
	title = {From LATE to MTE: Alternative methods for the evaluation of policy interventions},
	journal = {Labour Economics},
	volume = {41},
	pages = {47-60},
	year = {2016},
	note = {SOLE/EALE conference issue 2015},
	issn = {0927-5371},
	doi = {https://doi.org/10.1016/j.labeco.2016.06.004},
	url = {https://www.sciencedirect.com/science/article/pii/S0927537116300562},
	author = {Thomas Cornelissen and Christian Dustmann and Anna Raute and Uta Schönberg},
	keywords = {Marginal treatment effects, Instrumental variables, Heterogeneous effects},
	abstract = {This paper provides an introduction into the estimation of marginal treatment effects (MTE). Compared to the existing surveys on the subject, our paper is less technical and speaks to the applied economist with a solid basic understanding of econometric techniques who would like to use MTE estimation. Our framework of analysis is a generalized Roy model based on the potential outcomes framework, within which we define different treatment effects of interest, and review the well-known case of IV estimation with a discrete instrument resulting in a local average treatment effect (LATE). Turning to IV estimation with a continuous instrument, we demonstrate that the 2SLS estimator may be viewed as a weighted average of LATEs and discuss MTE estimation as an alternative and more informative way of exploiting a continuous instrument. We clarify the assumptions underlying the MTE framework, its relation to the correlated random coefficients model, and illustrate how the MTE estimation is implemented in practice.}
}

@article{Chernozhukov2018,
	author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
	title = "{Double/debiased machine learning for treatment and structural parameters}",
	journal = {The Econometrics Journal},
	volume = {21},
	number = {1},
	pages = {C1-C68},
	year = {2018},
	month = {01},
	abstract = "{We revisit the classic semi‐parametric problem of inference on a low‐dimensional parameter θ0 in the presence of high‐dimensional nuisance parameters η0. We depart from the classical setting by allowing for η0 to be so high‐dimensional that the traditional assumptions (e.g. Donsker properties) that limit complexity of the parameter space for this object break down. To estimate η0, we consider the use of statistical or machine learning (ML) methods, which are particularly well suited to estimation in modern, very high‐dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating η0 cause a heavy bias in estimators of θ0 that are obtained by naively plugging ML estimators of η0 into estimating equations for θ0. This bias results in the naive estimator failing to be N−1/2 consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest θ0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman‐orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate θ0; (2) making use of cross‐fitting, which provides an efficient form of data‐splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in an N−1/2‐neighbourhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements, which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters, such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of the following: DML applied to learn the main regression parameter in a partially linear regression model; DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model; DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness; DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.}",
	issn = {1368-4221},
	doi = {10.1111/ectj.12097},
	url = {https://doi.org/10.1111/ectj.12097},
	eprint = {https://academic.oup.com/ectj/article-pdf/21/1/C1/27684918/ectj00c1.pdf},
}


@article{Belloni2012,
	author = {Belloni, A. and Chen, D. and Chernozhukov, V. and Hansen, C.},
	title = {Sparse Models and Methods for Optimal Instruments With an Application to Eminent Domain},
	journal = {Econometrica},
	volume = {80},
	number = {6},
	pages = {2369-2429},
	keywords = {Inference on a low-dimensional parameter after model selection, imperfect model selection, instrumental variables, Lasso, post-Lasso, data-driven penalty, heteroscedasticity, non-Gaussian errors, moderate deviations for self-normalized sums},
	doi = {https://doi.org/10.3982/ECTA9626},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA9626},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.3982/ECTA9626},
	abstract = {We develop results for the use of Lasso and post-Lasso methods to form first-stage predictions and estimate optimal instruments in linear instrumental variables (IV) models with many instruments, p. Our results apply even when p is much larger than the sample size, n. We show that the IV estimator based on using Lasso or post-Lasso in the first stage is root-n consistent and asymptotically normal when the first stage is approximately sparse, that is, when the conditional expectation of the endogenous variables given the instruments can be well-approximated by a relatively small set of variables whose identities may be unknown. We also show that the estimator is semiparametrically efficient when the structural error is homoscedastic. Notably, our results allow for imperfect model selection, and do not rely upon the unrealistic “beta-min” conditions that are widely used to establish validity of inference following model selection (see also Belloni, Chernozhukov, and Hansen (2011b)). In simulation experiments, the Lasso-based IV estimator with a data-driven penalty performs well compared to recently advocated many-instrument robust procedures. In an empirical example dealing with the effect of judicial eminent domain decisions on economic outcomes, the Lasso-based IV estimator outperforms an intuitive benchmark. Optimal instruments are conditional expectations. In developing the IV results, we establish a series of new results for Lasso and post-Lasso estimators of nonparametric conditional expectation functions which are of independent theoretical and practical interest. We construct a modification of Lasso designed to deal with non-Gaussian, heteroscedastic disturbances that uses a data-weighted ℓ1-penalty function. By innovatively using moderate deviation theory for self-normalized sums, we provide convergence rates for the resulting Lasso and post-Lasso estimators that are as sharp as the corresponding rates in the homoscedastic Gaussian case under the condition that logp = o(n1/3). We also provide a data-driven method for choosing the penalty level that must be specified in obtaining Lasso and post-Lasso estimates and establish its asymptotic validity under non-Gaussian, heteroscedastic disturbances.},
	year = {2012}
}

@misc{caetano2023differenceindifferences,
	title={Difference-in-Differences with Time-Varying Covariates in the Parallel Trends Assumption}, 
	author={Carolina Caetano and Brantly Callaway},
	year={2023},
	eprint={2202.02903},
	archivePrefix={arXiv},
	primaryClass={econ.EM}
}

@article{Angrist1998,
	ISSN = {00129682, 14680262},
	URL = {http://www.jstor.org/stable/2998558},
	abstract = {The volunteer armed forces play a major role in the American youth labor market, but little is known about the effects of voluntary military service on earnings. The effects of military service are difficult to measure because veterans are both self-selected and screened by the military. This study uses two strategies to reduce selection bias in estimates of the effects of military service on the earnings of veterans. Both approaches involve the analysis of a special match of Social Security earning records to administrative data on applicants to the armed forces. The first strategy compares applicants who enlisted with applicants who did not enlist, while controlling for most of the characteristics used by the military to select soldiers from the applicant pool. This is implemented using matching methods and regression. The second strategy uses instrumental variables that were generated by an error in the scoring of the exams that screen military applicants. Estimates from both strategies are interpreted using models with heterogeneous potential outcomes. The empirical results suggest that soldiers who served in the early 1980s were paid considerably more than comparable civilians while in the military, and that military service is associated with higher employment rates for veterans after service. In spite of this employment gain, however, military service led to only a modest long-run increase in the civilian earnings of nonwhite veterans while actually reducing the civilian earnings of white veterans.},
	author = {Joshua D. Angrist},
	journal = {Econometrica},
	number = {2},
	pages = {249--288},
	publisher = {[Wiley, Econometric Society]},
	title = {Estimating the Labor Market Impact of Voluntary Military Service Using Social Security Data on Military Applicants},
	urldate = {2024-03-05},
	volume = {66},
	year = {1998}
}

@misc{goldsmithpinkham2024contamination,
	title={Contamination Bias in Linear Regressions}, 
	author={Paul Goldsmith-Pinkham and Peter Hull and Michal Kolesár},
	year={2024},
	eprint={2106.05024},
	archivePrefix={arXiv},
	primaryClass={econ.EM}
}

@article{Freyberger2018,
	title = {Uniform confidence bands: Characterization and optimality},
	journal = {Journal of Econometrics},
	volume = {204},
	number = {1},
	pages = {119-130},
	year = {2018},
	issn = {0304-4076},
	doi = {https://doi.org/10.1016/j.jeconom.2018.01.006},
	url = {https://www.sciencedirect.com/science/article/pii/S0304407618300174},
	author = {Joachim Freyberger and Yoshiyasu Rai},
	keywords = {Uniform confidence bands, Simultaneous inference, Projections, Optimality},
	abstract = {This paper studies optimal uniform confidence bands for functions g(x,β0), where β0 is an unknown parameter vector. We provide a simple characterization of a general class of taut 1−α uniform confidence bands, allowing for both nonlinear functions and nonparametrically estimated functions. Specifically, we show that all taut bands can be obtained from projections on confidence sets for β0 and we characterize the class of sets which yield taut bands. Using these results, we then present a computational method for selecting an approximately optimal confidence band for a given objective function. We illustrate the applicability of these results in numerical applications.}
}

@article{Mikusheva2021,
	author = {Mikusheva, Anna and Sun, Liyang},
	title = "{Inference with Many Weak Instruments}",
	journal = {The Review of Economic Studies},
	volume = {89},
	number = {5},
	pages = {2663-2686},
	year = {2021},
	month = {12},
	abstract = "{We develop a concept of weak identification in linear instrumental variable models in which the number of instruments can grow at the same rate or slower than the sample size. We propose a jackknifed version of the classical weak identification-robust Anderson–Rubin (AR) test statistic. Large-sample inference based on the jackknifed AR is valid under heteroscedasticity and weak identification. The feasible version of this statistic uses a novel variance estimator. The test has uniformly correct size and good power properties. We also develop a pre-test for weak identification that is related to the size property of a Wald test based on the Jackknife Instrumental Variable Estimator. This new pre-test is valid under heteroscedasticity and with many instruments.}",
	issn = {0034-6527},
	doi = {10.1093/restud/rdab097},
	url = {https://doi.org/10.1093/restud/rdab097},
	eprint = {https://academic.oup.com/restud/article-pdf/89/5/2663/45764025/rdab097.pdf},
}




@article{Haavelmo1944,
	ISSN = {00129682, 14680262},
	URL = {http://www.jstor.org/stable/1906935},
	author = {Trygve Haavelmo},
	journal = {Econometrica},
	pages = {iii--115},
	publisher = {[Wiley, Econometric Society]},
	title = {The Probability Approach in Econometrics},
	urldate = {2024-08-06},
	volume = {12},
	year = {1944}
}

@article{blanchard1989dynamic,
	title={The Dynamic Effects of Aggregate Demand and Supply Disturbances},
	author={Blanchard, Olivier Jean and Quah, Danny},
	journal={The American Economic Review},
	volume={79},
	number={4},
	pages={655--673},
	year={1989}
}

@article{Wright2012,
	author = {Wright, Jonathan H.},
	title = {What does Monetary Policy do to Long-term Interest Rates at the Zero Lower Bound?*},
	journal = {The Economic Journal},
	volume = {122},
	number = {564},
	pages = {F447-F466},
	doi = {https://doi.org/10.1111/j.1468-0297.2012.02556.x},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-0297.2012.02556.x},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1468-0297.2012.02556.x},
	abstract = {This article uses a structural VAR with daily data to identify the effects of monetary policy shocks on various longer term interest rates since the federal funds rate has been stuck at the zero lower bound. The VAR is identified using the assumption that monetary policy shocks are heteroskedastic: monetary policy shocks have especially high variance on days of FOMC meetings and certain speeches, while there is otherwise nothing unusual about these days. A complementary high-frequency event-study approach is also used. I find that stimulative monetary policy shocks lower Treasury and corporate bond yields but the effects die off fairly fast.},
	year = {2012}
}



@article{Miranda2021,
	Author = {Miranda-Agrippino, Silvia and Ricco, Giovanni},
	Title = {The Transmission of Monetary Policy Shocks},
	Journal = {American Economic Journal: Macroeconomics},
	Volume = {13},
	Number = {3},
	Year = {2021},
	Month = {July},
	Pages = {74-107},
	DOI = {10.1257/mac.20180124},
	URL = {https://www.aeaweb.org/articles?id=10.1257/mac.20180124}}
	
@article{Romer2004,
	Author = {Romer, Christina D. and Romer, David H.},
	Title = {A New Measure of Monetary Shocks: Derivation and Implications},
	Journal = {American Economic Review},
	Volume = {94},
	Number = {4},
	Year = {2004},
	Month = {September},
	Pages = {1055-1084},
	DOI = {10.1257/0002828042002651},
	URL = {https://www.aeaweb.org/articles?id=10.1257/0002828042002651}}
	
	@article{Gertler2015,
		Author = {Gertler, Mark and Karadi, Peter},
		Title = {Monetary Policy Surprises, Credit Costs, and Economic Activity},
		Journal = {American Economic Journal: Macroeconomics},
		Volume = {7},
		Number = {1},
		Year = {2015},
		Month = {January},
		Pages = {44-76},
		DOI = {10.1257/mac.20130329},
		URL = {https://www.aeaweb.org/articles?id=10.1257/mac.20130329}}

@article{plagborg2021,
	author = {Plagborg-Møller, Mikkel and Wolf, Christian K.},
	title = {Local Projections and VARs Estimate the Same Impulse Responses},
	journal = {Econometrica},
	volume = {89},
	number = {2},
	pages = {955-980},
	keywords = {External instrument, impulse response function, local projection, proxy variable, structural vector autoregression},
	doi = {https://doi.org/10.3982/ECTA17813},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA17813},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.3982/ECTA17813},
	abstract = {We prove that local projections (LPs) and Vector Autoregressions (VARs) estimate the same impulse responses. This nonparametric result only requires unrestricted lag structures. We discuss several implications: (i) LP and VAR estimators are not conceptually separate procedures; instead, they are simply two dimension reduction techniques with common estimand but different finite-sample properties. (ii) VAR-based structural identification—including short-run, long-run, or sign restrictions—can equivalently be performed using LPs, and vice versa. (iii) Structural estimation with an instrument (proxy) can be carried out by ordering the instrument first in a recursive VAR, even under noninvertibility. (iv) Linear VARs are as robust to nonlinearities as linear LPs.},
	year = {2021}
}



@article{uhlig2005,
	title = {What are the effects of monetary policy on output? Results from an agnostic identification procedure},
	journal = {Journal of Monetary Economics},
	volume = {52},
	number = {2},
	pages = {381-419},
	year = {2005},
	issn = {0304-3932},
	doi = {https://doi.org/10.1016/j.jmoneco.2004.05.007},
	url = {https://www.sciencedirect.com/science/article/pii/S0304393205000073},
	author = {Harald Uhlig},
	keywords = {Vector autoregression, Monetary policy shocks, Identification, Monetary neutrality},
	abstract = {This paper proposes to estimate the effects of monetary policy shocks by a new agnostic method, imposing sign restrictions on the impulse responses of prices, nonborrowed reserves and the federal funds rate in response to a monetary policy shock. No restrictions are imposed on the response of real GDP to answer the key question in the title. I find that “contractionary” monetary policy shocks have no clear effect on real GDP, even though prices move only gradually in response to a monetary policy shock. Neutrality of monetary policy shocks is not inconsistent with the data.}
}

@article{antolin2018,
	Author = {Antolín-Díaz, Juan and Rubio-Ramírez, Juan F.},
	Title = {Narrative Sign Restrictions for SVARs},
	Journal = {American Economic Review},
	Volume = {108},
	Number = {10},
	Year = {2018},
	Month = {October},
	Pages = {2802-29},
	DOI = {10.1257/aer.20161852},
	URL = {https://www.aeaweb.org/articles?id=10.1257/aer.20161852}}


@article{Christiano2005,
	ISSN = {00223808, 1537534X},
	URL = {http://www.jstor.org/stable/10.1086/426038},
	abstract = {We present a model embodying moderate amounts of nominal rigidities that accounts for the observed inertia in inflation and persistence in output. The key features of our model are those that prevent a sharp rise in marginal costs after an expansionary shock to monetary policy. Of these features, the most important are staggered wage contracts that have an average duration of three quarters and variable capital utilization.},
	author = {Lawrence J. Christiano and Martin Eichenbaum and Charles L. Evans},
	journal = {Journal of Political Economy},
	number = {1},
	pages = {1--45},
	publisher = {The University of Chicago Press},
	title = {Nominal Rigidities and the Dynamic Effects of a Shock to Monetary Policy},
	urldate = {2024-05-23},
	volume = {113},
	year = {2005}
}

@article{Sims1990,
	ISSN = {00129682, 14680262},
	URL = {http://www.jstor.org/stable/2938337},
	abstract = {This paper considers estimation and hypothesis testing in linear time series models when some or all of the variables have unit roots. Our motivating example is a vector autoregression with some unit roots in the companion matrix, which might include polynomials in time as regressors. In the general formulation, the variable might be integrated or cointegrated of arbitrary orders, and might have drifts as well. We show that parameters that can be written as coefficients on mean zero, nonintegrated regressors have jointly normal asymptotic distributions, converging at the rate T1/2. In general, the other coefficients (including the coefficients on polynomials in time) will have nonnormal asymptotic distributions. The results provide a formal characterization of which t or F tests--such as Granger causality tests--will be asymptotically valid, and which will have nonstandard limiting distributions.},
	author = {Christopher A. Sims and James H. Stock and Mark W. Watson},
	journal = {Econometrica},
	number = {1},
	pages = {113--144},
	publisher = {[Wiley, Econometric Society]},
	title = {Inference in Linear Time Series Models with some Unit Roots},
	urldate = {2024-05-15},
	volume = {58},
	year = {1990}
}

@book{Johansen1995,
	author = {Johansen, Søren},
	title = "{Likelihood-Based Inference in Cointegrated Vector Autoregressive Models}",
	publisher = {Oxford University Press},
	year = {1995},
	month = {12},
	abstract = "{This monograph is concerned with the statistical analysis of multivariate systems of non‐stationary time series of type I(1). It applies the concepts of cointegration and common trends in the framework of the Gaussian vector autoregressive model. The main result on the structure of cointegrated processes as defined by the error correction model is Grangers representation theorem. The statistical results include derivation of the trace test for cointegrating rank, test on cointegrating relations, and test on adjustment coefficients and their asymptotic distributions.}",
	isbn = {9780198774501},
	doi = {10.1093/0198774508.001.0001},
	url = {https://doi.org/10.1093/0198774508.001.0001},
}




@article{Johansen1991,
	ISSN = {00129682, 14680262},
	URL = {http://www.jstor.org/stable/2938278},
	abstract = {The purpose of this paper is to present the likelihood methods for the analysis of cointegration in VAR models with Gaussian errors, seasonal dummies, and constant terms. We discuss likelihood ratio tests of cointegration rank and find the asymptotic distribution of the test statistics. We characterize the maximum likelihood estimator of the cointegrating relations and formulate tests of structural hypotheses about these relations. We show that the asymptotic distribution of the maximum likelihood estimator is mixed Gaussian. Once a certain eigenvalue problem is solved and the eigenvectors and eigenvalues calculated, one can conduct inference on the cointegrating rank using some nonstandard distributions, and test hypotheses about cointegrating relations using the χ 2 distribution.},
	author = {Søren Johansen},
	journal = {Econometrica},
	number = {6},
	pages = {1551--1580},
	publisher = {[Wiley, Econometric Society]},
	title = {Estimation and Hypothesis Testing of Cointegration Vectors in Gaussian Vector Autoregressive Models},
	urldate = {2024-05-06},
	volume = {59},
	year = {1991}
}

@article{Chernozhukov2021,
	title = {Causal impact of masks, policies, behavior on early covid-19 pandemic in the U.S.},
	journal = {Journal of Econometrics},
	volume = {220},
	number = {1},
	pages = {23-62},
	year = {2021},
	note = {Pandemic Econometrics},
	issn = {0304-4076},
	doi = {https://doi.org/10.1016/j.jeconom.2020.09.003},
	url = {https://www.sciencedirect.com/science/article/pii/S0304407620303468},
	author = {Victor Chernozhukov and Hiroyuki Kasahara and Paul Schrimpf},
	keywords = {Covid-19, Causal impact, Masks, Policies, Behavior},
	abstract = {The paper evaluates the dynamic impact of various policies adopted by US states on the growth rates of confirmed Covid-19 cases and deaths as well as social distancing behavior measured by Google Mobility Reports, where we take into consideration people’s voluntarily behavioral response to new information of transmission risks in a causal structural model framework. Our analysis finds that both policies and information on transmission risks are important determinants of Covid-19 cases and deaths and shows that a change in policies explains a large fraction of observed changes in social distancing behavior. Our main counterfactual experiments suggest that nationally mandating face masks for employees early in the pandemic could have reduced the weekly growth rate of cases and deaths by more than 10 percentage points in late April and could have led to as much as 19 to 47 percent less deaths nationally by the end of May, which roughly translates into 19 to 47 thousand saved lives. We also find that, without stay-at-home orders, cases would have been larger by 6 to 63 percent and without business closures, cases would have been larger by 17 to 78 percent. We find considerable uncertainty over the effects of school closures due to lack of cross-sectional variation; we could not robustly rule out either large or small effects. Overall, substantial declines in growth rates are attributable to private behavioral response, but policies played an important role as well. We also carry out sensitivity analyses to find neighborhoods of the models under which the results hold robustly: the results on mask policies appear to be much more robust than the results on business closures and stay-at-home orders. Finally, we stress that our study is observational and therefore should be interpreted with great caution. From a completely agnostic point of view, our findings uncover predictive effects (association) of observed policies and behavioral changes on future health outcomes, controlling for informational and other confounding variables.}
}

@article{Newey1987,
	ISSN = {00129682, 14680262},
	URL = {http://www.jstor.org/stable/1913610},
	author = {Whitney K. Newey and Kenneth D. West},
	journal = {Econometrica},
	number = {3},
	pages = {703--708},
	publisher = {[Wiley, Econometric Society]},
	title = {A Simple, Positive Semi-Definite, Heteroskedasticity and Autocorrelation Consistent Covariance Matrix},
	urldate = {2024-03-13},
	volume = {55},
	year = {1987}
}

@article{Elliott1996,
	ISSN = {00129682, 14680262},
	URL = {http://www.jstor.org/stable/2171846},
	abstract = {The asymptotic power envelope is derived for point-optimal tests of a unit root in the autoregressive representation of a Gaussian time series under various trend specifications. We propose a family of tests whose asymptotic power functions are tangent to the power envelope at one point and are never far below the envelope. When the series has no deterministic component, some previously proposed tests are shown to be asymptotically equivalent to members of this family. When the series has an unknown mean or linear trend, commonly used tests are found to be dominated by members of the family of point-optimal invariant tests. We propose a modified version of the Dickey-Fuller t test which has substantially improved power when an unknown mean or trend is present. A Monte Carlo experiment indicates that the modified test works well in small samples.},
	author = {Graham Elliott and Thomas J. Rothenberg and James H. Stock},
	journal = {Econometrica},
	number = {4},
	pages = {813--836},
	publisher = {[Wiley, Econometric Society]},
	title = {Efficient Tests for an Autoregressive Unit Root},
	urldate = {2024-03-13},
	volume = {64},
	year = {1996}
}


@article{Phillips1988,
	ISSN = {00063444},
	URL = {http://www.jstor.org/stable/2336182},
	abstract = {This paper proposes new tests for detecting the presence of a unit root in quite general time series models. Our approach is nonparametric with respect to nuisance parameters and thereby allows for a very wide class of weakly dependent and possibly heterogeneously distributed data. The tests accommodate models with a fitted drift and a time trend so that they may be used to discriminate between unit root nonstationarity and stationarity about a deterministic trend. The limiting distributions of the statistics are obtained under both the unit root null and a sequence of local alternatives. The latter noncentral distribution theory yields local asymptotic power functions for the tests and facilitates comparisons with alternative procedures due to Dickey & Fuller. Simulations are reported on the performance of the new tests in finite samples.},
	author = {Peter C. B. Phillips and Pierre Perron},
	journal = {Biometrika},
	number = {2},
	pages = {335--346},
	publisher = {[Oxford University Press, Biometrika Trust]},
	title = {Testing for a Unit Root in Time Series Regression},
	urldate = {2024-03-13},
	volume = {75},
	year = {1988}
}


@article{Beveridge1981,
	title = {A new approach to decomposition of economic time series into permanent and transitory components with particular attention to measurement of the ‘business cycle’},
	journal = {Journal of Monetary Economics},
	volume = {7},
	number = {2},
	pages = {151-174},
	year = {1981},
	issn = {0304-3932},
	doi = {https://doi.org/10.1016/0304-3932(81)90040-4},
	url = {https://www.sciencedirect.com/science/article/pii/0304393281900404},
	author = {Stephen Beveridge and Charles R. Nelson},
	abstract = {This paper introduces a general procedure for decomposition of non-stationary time series into a permanent and transitory component allowing both components to be stochastic. The permanent component is shown to be a random walk with drift and the transitory or cyclical component is a stationary process with mean zero. The decomposition methodology, which depends only on past data and therefore is computable in ‘real time’, is applied to the problem of measuring and dating business ‘cycles’ in the portwar U.S. economy. We find that measured expansions and contractions are of roughly equivalent duration and that our dating of cyclical episodes tends to lead the traditional NBER dating and, to a lesser extent, the ‘growth cycle’ chronology of Zarnowitz and Boschan (1977).}
}

@BOOK{Shumway2017,
	title     = "Time series analysis and its applications",
	author    = "Shumway, Robert H and Stoffer, David S",
	publisher = "Springer International Publishing",
	series    = "Springer Texts in Statistics",
	edition   =  4,
	month     =  apr,
	year      =  2017,
	address   = "Basel, Switzerland",
	language  = "en"
}

@BOOK{Tsay2013,
	title     = "Multivariate time series analysis",
	author    = "Tsay, Ruey S",
	publisher = "John Wiley \& Sons",
	series    = "Wiley Series in Probability and Statistics",
	month     =  nov,
	year      =  2013,
	address   = "Nashville, TN",
	language  = "en"
}

@BOOK{Tsay2010,
	title     = "Analysis of financial time series",
	author    = "Tsay, Ruey S",
	publisher = "Wiley-Blackwell",
	edition   =  3,
	month     =  aug,
	year      =  2010,
	address   = "Hoboken, NJ",
	language  = "en"
}

@BOOK{Tsay2012,
	title     = "An introduction to analysis of financial data with {R}",
	author    = "Tsay, Ruey S",
	publisher = "Wiley-Blackwell",
	series    = "Wiley Series in Probability and Statistics",
	month     =  oct,
	year      =  2012,
	address   = "Hoboken, NJ",
	language  = "en"
}





@BOOK{Enders2014,
	title     = "Applied Econometric Time Series",
	author    = "Enders, Walter",
	publisher = "John Wiley \& Sons",
	series    = "Wiley Series in Probability and Statistics",
	edition   =  4,
	month     =  oct,
	year      =  2014,
	address   = "Nashville, TN"
}

@BOOK{Hamilton1994,
	title     = "Time series analysis",
	author    = "Hamilton, James Douglas",
	publisher = "Princeton University Press",
	month     =  jan,
	year      =  1994,
	address   = "Princeton, NJ",
	language  = "en"
}



@article{Carrasco2002,
	ISSN = {02664666, 14694360},
	URL = {http://www.jstor.org/stable/3533024},
	abstract = {This paper first provides some useful results on a generalized random coefficient autoregressive model and a generalized hidden Markov model. These results simultaneously imply strict stationarity, existence of higher order moments, geometric ergodicity, and β-mixing with exponential decay rates, which are important properties for statistical inference. As applications, we then provide easy-to-verify sufficient conditions to ensure β-mixing and finite higher order moments for various linear and nonlinear GARCH(1,1), linear and power GARCH(p,q), stochastic volatility, and autoregressive conditional duration models. For many of these models, our sufficient conditions for existence of second moments and exponential β-mixing are also necessary. For several GARCH(1,1) models, our sufficient conditions for existence of higher order moments again coincide with the necessary ones in He and Terasvirta.},
	author = {Marine Carrasco and Xiaohong Chen},
	journal = {Econometric Theory},
	number = {1},
	pages = {17--39},
	publisher = {Cambridge University Press},
	title = {Mixing and Moment Properties of Various GARCH and Stochastic Volatility Models},
	urldate = {2024-02-23},
	volume = {18},
	year = {2002}
}



@article{Heckman2022,
	author = "Heckman, James J. and Pinto, Rodrigo",
	title = "The Econometric Model for Causal Policy Analysis", 
	journal= "Annual Review of Economics",
	year = "2022",
	volume = "14",
	number = "Volume 14, 2022",
	pages = "893-923",
	doi = "https://doi.org/10.1146/annurev-economics-051520-015456",
	url = "https://www.annualreviews.org/content/journals/10.1146/annurev-economics-051520-015456",
	publisher = "Annual Reviews",
	issn = "1941-1391",
	type = "Journal Article",
	keywords = "policy analysis",
	keywords = "JEL C18",
	keywords = "econometric models",
	keywords = "JEL C10",
	keywords = "identification",
	keywords = "directed acyclic graphs",
	keywords = "treatment effects",
	keywords = "causal calculus",
	keywords = "causality",
	abstract = "This article discusses the econometric model of causal policy analysis and two alternative frameworks that are popular in statistics and computer science. By employing the alternative frameworks uncritically, economists ignore the substantial advantages of an econometric approach, and this results in less informative analyses of economic policy. We show that the econometric approach to causality enables economists to characterize and analyze a wider range of policy problems than is allowed by alternative approaches.",
}


@article{Heckman2024,
	title = {Econometric causality: The central role of thought experiments},
	journal = {Journal of Econometrics},
	volume = {243},
	number = {1},
	pages = {105719},
	year = {2024},
	issn = {0304-4076},
	doi = {https://doi.org/10.1016/j.jeconom.2024.105719},
	url = {https://www.sciencedirect.com/science/article/pii/S0304407624000654},
	author = {James Heckman and Rodrigo Pinto},
	keywords = {Structural equation models, Causality, Causal inference, Directed acyclic graphs, Simultaneous causality},
	abstract = {This paper examines the econometric causal model and the interpretation of empirical evidence based on thought experiments that was developed by Ragnar Frisch and Trygve Haavelmo. We compare the econometric causal model with two currently popular causal frameworks: the Neyman–Rubin causal model and the Do-Calculus. The Neyman–Rubin causal model is based on the language of potential outcomes and was largely developed by statisticians. Instead of being based on thought experiments, it takes statistical experiments as its foundation. The Do-Calculus, developed by Judea Pearl and co-authors, relies on Directed Acyclic Graphs (DAGs) and is a popular causal framework in computer science and applied mathematics. We make the case that economists who uncritically use these frameworks often discard the substantial benefits of the econometric causal model to the detriment of more informative analyses. We illustrate the versatility and capabilities of the econometric framework using causal models developed in economics.}
}

@article{Dickey1979,
	ISSN = {01621459},
	URL = {http://www.jstor.org/stable/2286348},
	abstract = {Let n observations Y1, Y2, ..., Yn be generated by the model Yt = ρ Yt - 1 + et, where Y0 is a fixed constant and {et}t = 1n is a sequence of independent normal random variables with mean 0 and variance σ2. Properties of the regression estimator of ρ are obtained under the assumption that ρ = ± 1. Representations for the limit distributions of the estimator of ρ and of the regression t test are derived. The estimator of ρ and the regression t test furnish methods of testing the hypothesis that ρ = 1.},
	author = {David A. Dickey and Wayne A. Fuller},
	journal = {Journal of the American Statistical Association},
	number = {366},
	pages = {427--431},
	publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
	title = {Distribution of the Estimators for Autoregressive Time Series With a Unit Root},
	volume = {74},
	year = {1979}
}

@article{Dickey1981,
	ISSN = {00129682, 14680262},
	URL = {http://www.jstor.org/stable/1912517},
	abstract = {Let the time series Yt satisfy $Y_{t}=\alpha +\rho Y_{t-1}+e_{t}$, where Y1 is fixed and the et are normal independent (0, σ 2) random variables. The likelihood ratio test of the hypothesis that (α, ρ) = (0, 1) is investigated and a limit representation for the test statistic is presented. Percentage points for the limiting distribution and for finite sample distributions are estimated. The distribution of the least squares estimator of α is also discussed. A similar investigation is conducted for the model containing a time trend.},
	author = {David A. Dickey and Wayne A. Fuller},
	journal = {Econometrica},
	number = {4},
	pages = {1057--1072},
	publisher = {[Wiley, Econometric Society]},
	title = {Likelihood Ratio Statistics for Autoregressive Time Series with a Unit Root},
	volume = {49},
	year = {1981}
}

@article{Ng2001,
	author = {Ng, Serena and Perron, Pierre},
	title = {LAG Length Selection and the Construction of Unit Root Tests with Good Size and Power},
	journal = {Econometrica},
	volume = {69},
	number = {6},
	pages = {1519-1554},
	keywords = {Integrated processes, truncation lag, GLS detrending, information criteria},
	doi = {https://doi.org/10.1111/1468-0262.00256},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1468-0262.00256},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/1468-0262.00256},
	abstract = {It is widely known that when there are errors with a moving-average root close to −1, a high order augmented autoregression is necessary for unit root tests to have good size, but that information criteria such as the AIC and the BIC tend to select a truncation lag (k) that is very small. We consider a class of Modified Information Criteria (MIC) with a penalty factor that is sample dependent. It takes into account the fact that the bias in the sum of the autoregressive coefficients is highly dependent on k and adapts to the type of deterministic components present. We use a local asymptotic framework in which the moving-average root is local to −1 to document how the MIC performs better in selecting appropriate values of k. In Monte-Carlo experiments, the MIC is found to yield huge size improvements to the DFGLS and the feasible point optimal PT test developed in Elliott, Rothenberg, and Stock (1996). We also extend the M tests developed in Perron and Ng (1996) to allow for GLS detrending of the data. The MIC along with GLS detrended data yield a set of tests with desirable size and power properties.},
	year = {2001}
}

@article{DeLivera2012,
	author = {Alysha M. De Livera and Rob J. Hyndman and Ralph D. Snyder},
	title = {Forecasting Time Series With Complex Seasonal Patterns Using Exponential Smoothing},
	journal = {Journal of the American Statistical Association},
	volume = {106},
	number = {496},
	pages = {1513-1527},
	year = {2011},
	publisher = {Taylor & Francis},
	doi = {10.1198/jasa.2011.tm09771},
	
	
	URL = { 
	
	https://doi.org/10.1198/jasa.2011.tm09771
	
	
	
	},
	eprint = { 
	
	https://doi.org/10.1198/jasa.2011.tm09771
	
	
	
	}
	
}

@article{Ravn2002,
	author = {Ravn, Morten O. and Uhlig, Harald},
	title = "{On Adjusting the Hodrick-Prescott Filter for the Frequency of Observations}",
	journal = {The Review of Economics and Statistics},
	volume = {84},
	number = {2},
	pages = {371-376},
	year = {2002},
	month = {05},
	abstract = "{This paper studies how the Hodrick-Prescott filter should be adjusted when changing the frequency of observations. It complements the results of Baxter and King (1999) with an analytical analysis, demonstrating that the filter parameter should be adjusted by multiplying it with the fourth power of the observation frequency ratios. This yields an HP parameter value of 6.25 for annual data given a value of 1600 for quarterly data. The relevance of the suggestion is illustrated empirically.}",
	issn = {0034-6535},
	doi = {10.1162/003465302317411604},
	url = {https://doi.org/10.1162/003465302317411604},
	eprint = {https://direct.mit.edu/rest/article-pdf/84/2/371/1613390/003465302317411604.pdf},
}

@article{Said1984,
	ISSN = {00063444},
	URL = {http://www.jstor.org/stable/2336570},
	abstract = {Recently, methods for detecting unit roots in autoregressive and autoregressive-moving average time series have been proposed. The presence of a unit root indicates that the time series is not stationary but that differencing will reduce it to stationarity. The tests proposed to data require specification of the number of autoregressive and moving average coefficients in the model. In this paper we develop a test for unit roots which is based on an approximation of an autoregressive-moving average model by an autoregression. The test statistic is standard output from most regression programs and has a limit distribution whose percentiles have been tabulated. An example is provided.},
	author = {Said E. Said and David A. Dickey},
	journal = {Biometrika},
	number = {3},
	pages = {599--607},
	publisher = {[Oxford University Press, Biometrika Trust]},
	title = {Testing for Unit Roots in Autoregressive-Moving Average Models of Unknown Order},
	volume = {71},
	year = {1984}
}




@article{Hamilton2019,
	author = {Hamilton, James D.},
	title = "{Why You Should Never Use the Hodrick-Prescott Filter}",
	journal = {The Review of Economics and Statistics},
	volume = {100},
	number = {5},
	pages = {831-843},
	year = {2018},
	month = {12},
	abstract = "{Here’s why. (a) The Hodrick-Prescott (HP) filter introduces spurious dynamic relations that have no basis in the underlying data-generating process. (b) Filtered values at the end of the sample are very different from those in the middle and are also characterized by spurious dynamics. (c) A statistical formalization of the problem typically produces values for the smoothing parameter vastly at odds with common practice. (d) There is a better alternative. A regression of the variable at date t on the four most recent values as of date t - h achieves all the objectives sought by users of the HP filter with none of its drawbacks.}",
	issn = {0034-6535},
	doi = {10.1162/rest_a_00706},
	url = {https://doi.org/10.1162/rest\_a\_00706},
	eprint = {https://direct.mit.edu/rest/article-pdf/100/5/831/1918879/rest\_a\_00706.pdf},
}
 
 

@article{Dickey1987,
	ISSN = {07350015},
	URL = {http://www.jstor.org/stable/1391997},
	abstract = {One way of handling nonstationarity in time series is to compute first differences and fit a model to the differenced series unless the differenced series also looks nonstationary. In that case, second- or higher-order differencing is done. To decide if the current degree of differencing is sufficient, one can look at the autocorrelation function for slow decay. A formal statistical test for the need to difference further is available if one is willing to assume that at most one more difference will render the series stationary. In this article, we present a proper sequence of statistical tests that allows the practitioner to handle cases in which a high order of differencing may be needed. The proper sequence is not the traditional sequence, which begins with a test for a single unit root.},
	author = {David A. Dickey and Sastry G. Pantula},
	journal = {Journal of Business \& Economic Statistics},
	number = {4},
	pages = {455--461},
	publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
	title = {Determining the Order of Differencing in Autoregressive Processes},
	urldate = {2024-03-17},
	volume = {5},
	year = {1987}
}






@article{Diebold2015,
	author = {Francis X. Diebold},
	title = {Comparing Predictive Accuracy, Twenty Years Later: A Personal Perspective on the Use and Abuse of Diebold–Mariano Tests},
	journal = {Journal of Business \& Economic Statistics},
	volume = {33},
	number = {1},
	pages = {1--1},
	year = {2015},
	publisher = {Taylor \& Francis},
	doi = {10.1080/07350015.2014.983236},
	
	
	URL = { 
	
	https://doi.org/10.1080/07350015.2014.983236
	
	
	
	},
	eprint = { 
	
	https://doi.org/10.1080/07350015.2014.983236
	
	
	
	}
	
}


@article{Menchetti2022,
	author = {Menchetti, Fiammetta and Cipollini, Fabrizio and Mealli, Fabrizia},
	title = "{Combining counterfactual outcomes and ARIMA models for policy evaluation}",
	journal = {The Econometrics Journal},
	volume = {26},
	number = {1},
	pages = {1-24},
	year = {2022},
	month = {09},
	abstract = "{The Rubin Causal Model (RCM) is a framework that allows to define the causal effect of an intervention as a contrast of potential outcomes. In recent years, several methods have been developed under the RCM to estimate causal effects in time series settings. None of these makes use of autoregressive integrated moving average (ARIMA) models, which are instead very common in the econometrics literature. In this paper, we propose a novel approach, named Causal-ARIMA (C-ARIMA), to define and estimate the causal effect of an intervention in observational time series settings under the RCM. We first formalise the assumptions enabling the definition, the estimation and the attribution of the effect to the intervention. We then check the validity of the proposed method with a simulation study. In the empirical application, we use C-ARIMA to assess the causal effect of a permanent price reduction on supermarket sales. The CausalArima R package provides an implementation of the proposed approach.}",
	issn = {1368-4221},
	doi = {10.1093/ectj/utac024},
	url = {https://doi.org/10.1093/ectj/utac024},
	eprint = {https://academic.oup.com/ectj/article-pdf/26/1/1/48597685/utac024.pdf},
}

@article{EngleGranger1987,
	ISSN = {00129682, 14680262},
	URL = {http://www.jstor.org/stable/1913236},
	abstract = {The relationship between co-integration and error correction models, first suggested in Granger (1981), is here extended and used to develop estimation procedures, tests, and empirical examples. If each element of a vector of time series xt first achieves stationarity after differencing, but a linear combination $\alpha ^{\prime }x_{t}$ is already stationary, the time series xt are said to be co-integrated with co-integrating vector α. There may be several such co-integrating vectors so that α becomes a matrix. Interpreting $\alpha ^{\prime }x_{t}=0$ as a long run equilibrium, co-integration implies that deviations from equilibrium are stationary, with finite variance, even though the series themselves are nonstationary and have infinite variance. The paper presents a representation theorem based on Granger (1983), which connects the moving average, autoregressive, and error correction representations for co-integrated systems. A vector autoregression in differenced variables is incompatible with these representations. Estimation of these models is discussed and a simple but asymptotically efficient two-step estimator is proposed. Testing for co-integration combines the problems of unit root tests and tests with parameters unidentified under the null. Seven statistics are formulated and analyzed. The critical values of these statistics are calculated based on a Monte Carlo simulation. Using these critical values, the power properties of the tests are examined and one test procedure is recommended for application. In a series of examples it is found that consumption and income are co-integrated, wages and prices are not, short and long interest rates are, and nominal GNP is co-integrated with M2, but not M1, M3, or aggregate liquid assets.},
	author = {Robert F. Engle and C. W. J. Granger},
	journal = {Econometrica},
	number = {2},
	pages = {251--276},
	publisher = {[Wiley, Econometric Society]},
	title = {Co-Integration and Error Correction: Representation, Estimation, and Testing},
	urldate = {2024-05-02},
	volume = {55},
	year = {1987}
}

@article{Phillips1990,
	ISSN = {00129682, 14680262},
	URL = {http://www.jstor.org/stable/2938339},
	abstract = {This paper develops an asymptotic theory for residual based tests for cointegration. These tests involve procedures that are designed to detect the presence of a unit root in the residuals of (cointegrating) regressions among the levels of economic time series. Attention is given to the augmented Dickey-Fuller (ADF) test that is recommended by Engle-Granger (1987) and the Zα and Zt unit root tests recently proposed by Phillips (1987). Two new tests are also introduced, one of which is invariant to the normalization of the cointegrating regression. All of these tests are shown to be asymptotically similar and simple representations of their limiting distributions are given in terms of standard Brownian motion. The ADF and Zt tests are asymptotically equivalent. Power properties of the tests are also studied. The analysis shows that all the tests are consistent if suitably constructed but that the ADF and Zt tests have slower rates of divergence under cointegration than the other tests. This indicates that, at least in large samples, the Zα test should have superior power properties. The paper concludes by addressing the larger issue of test formulation. Some major pitfalls are discovered in procedures that are designed to test a null of cointegration (rather than no cointegration). These defects provide strong arguments against the indiscriminate use of such test formulations and support the continuing use of residual based unit root tests. A full set of critical values for residual based tests is included. These allow for demeaned and detrended data and cointegrating regressions with up to five variables.},
	author = {P. C. B. Phillips and S. Ouliaris},
	journal = {Econometrica},
	number = {1},
	pages = {165--193},
	publisher = {[Wiley, Econometric Society]},
	title = {Asymptotic Properties of Residual Based Tests for Cointegration},
	urldate = {2024-05-02},
	volume = {58},
	year = {1990}
}


@article{Stock2018,
	author = {Stock, James H. and Watson, Mark W.},
	title = "{Identification and Estimation of Dynamic Causal Effects in Macroeconomics Using External Instruments}",
	journal = {The Economic Journal},
	volume = {128},
	number = {610},
	pages = {917-948},
	year = {2018},
	month = {05},
	abstract = "{External sources of as‐if randomness — that is, external instruments — can be used to identify the dynamic causal effects of macroeconomic shocks. One method is a one‐step instrumental variables regression (local projections – IV); a more efficient two‐step method involves a vector autoregression. We show that, under a restrictive instrument validity condition, the one‐step method is valid even if the vector autoregression is not invertible, so comparing the two estimates provides a test of invertibility. If, however, lagged endogenous variables are needed as control variables in the one‐step method, then the conditions for validity of the two methods are the same.}",
	issn = {0013-0133},
	doi = {10.1111/ecoj.12593},
	url = {https://doi.org/10.1111/ecoj.12593},
	eprint = {https://academic.oup.com/ej/article-pdf/128/610/917/26398109/ej0917.pdf},
}


@article{Alvarez2024,
	title = {The interpretation of 2SLS with a continuous instrument: A weighted LATE representation},
	journal = {Economics Letters},
	volume = {237},
	pages = {111658},
	year = {2024},
	issn = {0165-1765},
	doi = {https://doi.org/10.1016/j.econlet.2024.111658},
	url = {https://www.sciencedirect.com/science/article/pii/S0165176524001411},
	author = {Luis A.F. Alvarez and Rodrigo Toneto},
	keywords = {Instrumental variables, Local average treatment effects, Underlying weights},
	abstract = {This note introduces a novel weighted local average treatment effect representation for the two-stages least-squares (2SLS) estimand in the continuous instrument with binary treatment case. Under standard conditions, we obtain weights that are nonnegative, integrate to unity, and assign larger values to instrument support points that deviate from their average. Our representation does not require instruments to be discretized nor relies on limiting arguments, such as those used in the definition of the marginal treatment effect (MTE). The pattern of the weights also has a clear interpretation. We believe these features of the representation to be useful for applied researchers when communicating their results. As a direct byproduct of our approach, we also obtain a representation of the 2SLS estimand as a weighted average of treatment effects among “marginal compliance” groups, without having to resort to the threshold-crossing representation underlying the MTE construction. The latter representation has an intuitive interpretation as well.}
}

